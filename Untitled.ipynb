{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ae0d3a-9044-4fde-b2ab-4126cd6fe0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import argparse\n",
    "import ruamel_yaml as yaml\n",
    "import numpy as np\n",
    "from generation_api.metrics import compute_scores\n",
    "from generation_api.optimizers import build_optimizer_blip, build_lr_scheduler\n",
    "from generation_api.trainer_blip import Trainer\n",
    "from generation_api.loss import compute_loss\n",
    "from transformers import BertTokenizer\n",
    "from generation_api.tokenizers_blip import Tokenizer\n",
    "from models.blip import blip_decoder\n",
    "from blip_original import create_loader, create_dataset\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed\n",
    "from models.blip import *\n",
    "\n",
    "def load(args, config):\n",
    "    train_dataset, val_dataset, test_dataset = create_dataset('generation_%s'%args.dataset_name, args, config)\n",
    "    samplers = [None, None, None]\n",
    "    train_dataloader, val_dataloader, test_dataloader = create_loader([train_dataset, val_dataset, test_dataset], samplers,\n",
    "                                                            batch_size=[args.batch_size] * 3,\n",
    "                                                            num_workers=[4, 4, 4],\n",
    "                                                            is_trains=[True, False, False],\n",
    "                                                            collate_fns=[None, None, None])\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def main(args, config):\n",
    "    # torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # create tokenizer\n",
    "\n",
    "    if args.bert == 'base':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif args.bert == 'sci':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    elif args.bert == 'cli':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "    tokenizer.add_special_tokens({'bos_token': '[DEC]'})\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[ENC]']})\n",
    "    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]\n",
    "    # tokenizer = BertTokenizer.from_pretrained(args.text_encoder)\n",
    "\n",
    "    # TODO: check how to load vit checkpoint. I still could not find the loading program. Huggingface is ofc a solution if we need to write it ourself.\n",
    "    model = blip_decoder(pretrained=args.pretrained, image_size=config['image_size'], vit=config['vit'],\n",
    "                         vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'],\n",
    "                         prompt=config['prompt'], tokenizer=tokenizer, args=args)\n",
    "    \n",
    "    # loader\n",
    "    train_dataset, val_dataset, test_dataset = create_dataset('generation_%s'%args.dataset_name, args, config)\n",
    "    samplers = [None, None, None]\n",
    "    train_dataloader, val_dataloader, test_dataloader = create_loader([train_dataset, val_dataset, test_dataset], samplers,\n",
    "                                                            batch_size=[args.batch_size] * 3,\n",
    "                                                            num_workers=[4, 4, 4],\n",
    "                                                            is_trains=[True, False, False],\n",
    "                                                            collate_fns=[None, None, None])\n",
    "\n",
    "\n",
    "#     criterion = compute_loss\n",
    "#     metrics = compute_scores\n",
    "\n",
    "#     # build optimizer, learning rate scheduler\n",
    "#     optimizer = build_optimizer_blip(args, model)\n",
    "#     lr_scheduler = build_lr_scheduler(args, optimizer)\n",
    "\n",
    "#     # build trainer and start to train\n",
    "#     trainer = Trainer(model, criterion, metrics, optimizer, args, lr_scheduler, train_dataloader, val_dataloader, test_dataloader, tokenizer)\n",
    "#     trainer.train()\n",
    "    \n",
    "#     config = yaml.load(open('/DATA1/bzhu/DCL/configs/BLIP.yaml', 'r'), Loader=yaml.Loader)\n",
    "\n",
    "#     main(args, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0bdb4ee-c0b2-4404-a1ea-c7009b305ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/DATA1/bzhu/DCL/medical_knowledge')\n",
    "\n",
    "import logging\n",
    "from models.vit_blip import VisionTransformer, interpolate_pos_embed\n",
    "from models.med import BertConfig, BertModel, BertLMHeadModel\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from timm.models.hub import download_cached_file\n",
    "from functools import partial\n",
    "from medical_knowledge.knowledge import create_knowledge\n",
    "from medical_knowledge.SKG_knowledge import *\n",
    "from models.tagencoder import TagEncoder, update_skg\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "base_path = os.getcwd()\n",
    "        \n",
    "class BLIP_Decoder(nn.Module):\n",
    "    def __init__(self,                 \n",
    "                 med_config = os.path.join(base_path, 'configs/med_config.json'),  \n",
    "                 image_size = 224,\n",
    "                 vit = 'base',\n",
    "                 vit_grad_ckpt = False,\n",
    "                 vit_ckpt_layer = 0,\n",
    "                 prompt = 'a picture of ',\n",
    "                 tokenizer = None,\n",
    "                 embed_dim = 256,     \n",
    "                 queue_size = 65536,\n",
    "                 momentum = 0.995,\n",
    "                 args = None\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
    "            image_size (int): input image size\n",
    "            vit (str): model size of vision transformer\n",
    "        \"\"\"            \n",
    "        super().__init__()\n",
    "        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
    "        # self.tokenizer = init_tokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        if args.bert == 'base':\n",
    "            med_config = os.path.join(base_path, 'configs/med_config_blip.json')\n",
    "        elif args.bert == 'sci':\n",
    "            med_config = os.path.join(base_path, 'configs/med_config_sci.json')\n",
    "        elif args.bert == 'cli':\n",
    "            med_config = os.path.join(base_path, 'configs/med_config_cli.json')\n",
    "        med_config = BertConfig.from_json_file(med_config)\n",
    "        med_config.encoder_width = vision_width\n",
    "        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)\n",
    "\n",
    "        # med_config.encoder_width = vision_width\n",
    "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1\n",
    "        self.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "        text_width = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.text_decoder = BertLMHeadModel(config=med_config)\n",
    "        \n",
    "        self.vision_proj = nn.Linear(vision_width, 256)\n",
    "        self.text_proj = nn.Linear(768, 256)\n",
    "\n",
    "            \n",
    "        self.itm_head = nn.Linear(text_width, 2) \n",
    "\n",
    "        # create momentum encoders  \n",
    "        self.visual_encoder_m, vision_width = create_vit(vit,image_size)              \n",
    "        self.vision_proj_m = nn.Linear(vision_width, embed_dim)\n",
    "        self.text_encoder_m = BertModel(config=med_config, add_pooling_layer=False)      \n",
    "        self.text_proj_m = nn.Linear(text_width, embed_dim)\n",
    "        \n",
    "        c = copy.deepcopy\n",
    "        attn = MultiHeadedAttention(6, 768)\n",
    "        ff = PositionwiseFeedForward(768, 1024, 0.1)\n",
    "        self.cross_attn = Decoder(DecoderLayer(768, c(attn), c(ff), 0.1), 2)\n",
    "\n",
    "        self.tag_encoder = TagEncoder(0.1, self.args)\n",
    "\n",
    "        self.cross_attn_m = Decoder(DecoderLayer(768, c(attn), c(ff), 0.1), 2)\n",
    "        \n",
    "        if args.dataset_name == 'iu_xray':\n",
    "            self.iu_proj = nn.Linear(768*2, 768)\n",
    "            self.iu_proj_m = nn.Linear(768*2, 768)\n",
    "            queue_size = 1380\n",
    "            self.model_pairs = [[self.visual_encoder, self.visual_encoder_m],\n",
    "                            [self.vision_proj, self.vision_proj_m],\n",
    "                            [self.text_encoder, self.text_encoder_m],\n",
    "                            [self.text_proj, self.text_proj_m],\n",
    "                            [self.cross_attn, self.cross_attn_m],\n",
    "                            [self.iu_proj,self.iu_proj_m]]\n",
    "        else:\n",
    "            self.model_pairs = [[self.visual_encoder, self.visual_encoder_m],\n",
    "                            [self.vision_proj, self.vision_proj_m],\n",
    "                            [self.text_encoder, self.text_encoder_m],\n",
    "                            [self.text_proj, self.text_proj_m],\n",
    "                            [self.cross_attn, self.cross_attn_m]\n",
    "                            ]\n",
    "        \n",
    "\n",
    "        self.copy_params()\n",
    "        \n",
    "        # create the queue\n",
    "        self.register_buffer(\"image_queue\", torch.randn(embed_dim, queue_size))\n",
    "        self.register_buffer(\"text_queue\", torch.randn(embed_dim, queue_size))\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "\n",
    "        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)\n",
    "        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\n",
    "\n",
    "        self.queue_size = queue_size\n",
    "        self.momentum = momentum\n",
    "        self.temp = nn.Parameter(0.07*torch.ones([]))   \n",
    "        \n",
    "        # create the decoder\n",
    "        decoder_config = BertModel(config=med_config)\n",
    "        decoder_config.encoder_width = vision_width\n",
    "        if args.bert == 'base':\n",
    "            self.text_decoder = BertLMHeadModel.from_pretrained('bert-base-uncased',config=decoder_config)\n",
    "        elif args.bert == 'sci':\n",
    "            # self.text_decoder = BertLMHeadModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "            # self.text_decoder = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=decoder_config)\n",
    "            print(decoder_config)\n",
    "            self.text_decoder = BertLMHeadModel.from_pretrained('allenai/scibert_scivocab_uncased',config=decoder_config)\n",
    "            print(self.text_decoder)\n",
    "        elif args.bert == 'cli':\n",
    "            self.text_decoder = BertLMHeadModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT',config=decoder_config)\n",
    "        self.text_decoder.resize_token_embeddings(len(self.tokenizer))\n",
    "        tie_encoder_decoder_weights(self.text_encoder,self.text_decoder,'','/attention')\n",
    "\n",
    "        self.create_knowledge = create_knowledge(embed_dim=embed_dim, queue_size=queue_size,\n",
    "                                                 text_encoder=self.text_encoder, text_proj=self.text_proj,\n",
    "                                                 tokenizer=self.tokenizer, args=args)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, image, caption, knowledge_skg, knowledge_tc,alpha, epoch):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        with torch.no_grad():\n",
    "            self.temp.clamp_(0.001,0.5)\n",
    "        if self.args.dataset_name == 'iu_xray':\n",
    "            image_embeds0 = self.visual_encoder(image[:, 0])\n",
    "            image_embeds1 = self.visual_encoder(image[:, 1])\n",
    "            image_embeds = torch.cat((image_embeds0, image_embeds1), dim=2)\n",
    "            image_embeds = self.iu_proj(image_embeds)\n",
    "        else:\n",
    "            image_embeds = self.visual_encoder(image)\n",
    "\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "        image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "        text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=90, return_tensors=\"pt\").to(image.device)\n",
    "        text_output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                        return_dict=True, mode='text')\n",
    "        text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1)\n",
    "        self.create_knowledge(image.device, text_feat, knowledge_tc)\n",
    "        \n",
    "        ###============== Obtain Knowledge ===================###\n",
    "        if epoch > 0:\n",
    "            # ---------------------Get Knowledge Words---------------------\n",
    "            _, knowledge_words = self.create_knowledge.get_image_knowledge(image.device, image_feat, image_embeds, k=3)\n",
    "\n",
    "            #get each knowledge\n",
    "            bs = image_feat.shape[0]\n",
    "            knowledge_tc_re = []\n",
    "            for i in range(bs):\n",
    "                knowledge_all = ''\n",
    "                for j in range(3):\n",
    "                    knowledge_item = knowledge_words[i+bs*j]\n",
    "                    knowledge_item = knowledge_item.split('-')\n",
    "                    knowledge_item.pop()\n",
    "                    knowledge_item = '-'.join(knowledge_item)\n",
    "                    if j == 0:\n",
    "                        knowledge_all += knowledge_item\n",
    "                    else:\n",
    "                        knowledge_all = knowledge_all + '-' + knowledge_item\n",
    "                    # knowledge_all += knowledge_item.replace(knowledge_item.split('-')[-1], ' ')\n",
    "                knowledge_list = knowledge_all.split('-')\n",
    "                # knowledge_list.pop()\n",
    "                knowledge_tc_re.append(knowledge_list)\n",
    "                \n",
    "            knowledge_skg['node_inds'] = [each.tolist() for each in knowledge_skg['node_inds']]\n",
    "            knowledge_skg['node_labels'] = [each.tolist() for each in knowledge_skg['node_labels']]\n",
    "            knowledge_skg['node_relations'] = [each.tolist() for each in knowledge_skg['node_relations']]\n",
    "            for i in range(58):\n",
    "                knowledge_skg['node_inds'].append(torch.zeros(bs).tolist())\n",
    "                knowledge_skg['node_labels'].append(torch.zeros(bs).tolist())\n",
    "                knowledge_skg['node_relations'].append(torch.zeros(bs).tolist())\n",
    "            knowledge_skg['node_inds'] = torch.tensor(knowledge_skg['node_inds'])\n",
    "            knowledge_skg['node_labels'] = torch.tensor(knowledge_skg['node_labels'])\n",
    "            knowledge_skg['node_relations'] = torch.tensor(knowledge_skg['node_relations'])\n",
    "\n",
    "            for idx in range(len(knowledge_tc_re)):\n",
    "                knowledge = knowledge_tc_re[idx]\n",
    "                for i in range(len(knowledge)):\n",
    "                    knowledge[i] = knowledge[i].strip().replace('located _ at', 'located_at').replace('suggestive _ of', 'suggestive_of')\n",
    "                    triplet_item = knowledge[i].split(' ')\n",
    "                    if len(triplet_item) != 3:\n",
    "                        triplet = None\n",
    "                        knowledge_skg = update_skg(knowledge_skg, triplet, idx, i)\n",
    "                    else:\n",
    "                        triplet = [triplet_item[0], triplet_item[1], triplet_item[2]]\n",
    "                        knowledge_skg = update_skg(knowledge_skg, triplet, idx, i)\n",
    "            for i in range(bs):\n",
    "                knowledge_skg['nodes'][i] = knowledge_skg['nodes'][i].replace('-', ' ')\n",
    "        else:\n",
    "            bs = image_feat.shape[0]\n",
    "            knowledge_skg['node_inds'] = [each.tolist() for each in knowledge_skg['node_inds']]\n",
    "            knowledge_skg['node_labels'] = [each.tolist() for each in knowledge_skg['node_labels']]\n",
    "            knowledge_skg['node_relations'] = [each.tolist() for each in knowledge_skg['node_relations']]\n",
    "            for i in range(58):\n",
    "                knowledge_skg['node_inds'].append(torch.zeros(bs).tolist())\n",
    "                knowledge_skg['node_labels'].append(torch.zeros(bs).tolist())\n",
    "                knowledge_skg['node_relations'].append(torch.zeros(bs).tolist())\n",
    "            knowledge_skg['node_inds'] = torch.tensor(knowledge_skg['node_inds'])\n",
    "            knowledge_skg['node_labels'] = torch.tensor(knowledge_skg['node_labels'])\n",
    "            knowledge_skg['node_relations'] = torch.tensor(knowledge_skg['node_relations'])\n",
    "            for i in range(bs):\n",
    "                knowledge_skg['nodes'][i] = knowledge_skg['nodes'][i].replace('-', ' ')\n",
    "                \n",
    "        tag_output = self.tag_encoder(knowledge_skg, image.device)\n",
    "\n",
    "        image_embeds, vis_attn2 = self.cross_attn(image_embeds, tag_output)\n",
    "\n",
    "        image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # bs x 768     \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n",
    "        ###============== Image-report Contrastive Learning ===================###\n",
    "\n",
    "        # get momentum features\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update()\n",
    "            \n",
    "            if self.args.dataset_name == 'iu_xray':\n",
    "                image_embeds0_m = self.visual_encoder_m(image[:, 0])\n",
    "                image_embeds1_m = self.visual_encoder_m(image[:, 1])\n",
    "                image_embeds_m = torch.cat((image_embeds0_m, image_embeds1_m), dim=2)\n",
    "                image_embeds_m = self.iu_proj_m(image_embeds_m)\n",
    "            else:\n",
    "                image_embeds_m = self.visual_encoder_m(image)\n",
    "            \n",
    "            image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]),dim=-1)\n",
    "\n",
    "            text_output_m = self.text_encoder_m(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                                return_dict=True, mode='text')\n",
    "            text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1)\n",
    "\n",
    "            ###============== Obtain Knowledge ===================###\n",
    "\n",
    "            image_embeds_m, _ = self.cross_attn_m(image_embeds_m, tag_output)\n",
    "            image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1)\n",
    "\n",
    "            image_feat_all = torch.cat([image_feat_m.t(),self.image_queue.clone().detach()],dim=1)\n",
    "\n",
    "            text_feat_all = torch.cat([text_feat_m.t(),self.text_queue.clone().detach()],dim=1)\n",
    "\n",
    "            sim_i2t_m = image_feat_m @ text_feat_all / self.temp\n",
    "            sim_t2i_m = text_feat_m @ image_feat_all / self.temp\n",
    "\n",
    "            sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device)\n",
    "            sim_targets.fill_diagonal_(1)\n",
    "\n",
    "            sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets\n",
    "            sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets\n",
    "\n",
    "        sim_i2t = image_feat @ text_feat_all / self.temp\n",
    "        sim_t2i = text_feat @ image_feat_all / self.temp\n",
    "\n",
    "        loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_i2t_targets,dim=1).mean()\n",
    "        loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_t2i_targets,dim=1).mean()\n",
    "\n",
    "        loss_irc = (loss_i2t+loss_t2i)/2\n",
    "\n",
    "        self._dequeue_and_enqueue(image_feat_m, text_feat_m)\n",
    "\n",
    "        ###============== Image-report Matching ===================###\n",
    "        encoder_input_ids = text.input_ids.clone()\n",
    "        encoder_input_ids[:,0] = self.tokenizer.enc_token_id\n",
    "        \n",
    "        # forward the positve image-text pair\n",
    "        bs = image.size(0)\n",
    "        output_pos = self.text_encoder(encoder_input_ids,\n",
    "                                       attention_mask = text.attention_mask,\n",
    "                                       encoder_hidden_states = image_embeds,\n",
    "                                       encoder_attention_mask = image_atts,      \n",
    "                                       return_dict = True,\n",
    "                                      )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weights_t2i = F.softmax(sim_t2i[:,:bs],dim=1)+1e-4\n",
    "            weights_t2i.fill_diagonal_(0)\n",
    "            weights_i2t = F.softmax(sim_i2t[:,:bs],dim=1)+1e-4\n",
    "            weights_i2t.fill_diagonal_(0)\n",
    "\n",
    "        # select a negative image for each text\n",
    "        image_embeds_neg = []\n",
    "        for b in range(bs):\n",
    "            neg_idx = torch.multinomial(weights_t2i[b], 1).item()\n",
    "            image_embeds_neg.append(image_embeds[neg_idx])\n",
    "        image_embeds_neg = torch.stack(image_embeds_neg,dim=0)\n",
    "\n",
    "        # select a negative text for each image\n",
    "        text_ids_neg = []\n",
    "        text_atts_neg = []\n",
    "        for b in range(bs):\n",
    "            neg_idx = torch.multinomial(weights_i2t[b], 1).item()\n",
    "            text_ids_neg.append(encoder_input_ids[neg_idx])\n",
    "            text_atts_neg.append(text.attention_mask[neg_idx])\n",
    "\n",
    "        text_ids_neg = torch.stack(text_ids_neg,dim=0)\n",
    "        text_atts_neg = torch.stack(text_atts_neg,dim=0)\n",
    "\n",
    "        text_ids_all = torch.cat([encoder_input_ids, text_ids_neg],dim=0)\n",
    "        text_atts_all = torch.cat([text.attention_mask, text_atts_neg],dim=0)\n",
    "\n",
    "        image_embeds_all = torch.cat([image_embeds_neg,image_embeds],dim=0)\n",
    "        image_atts_all = torch.cat([image_atts,image_atts],dim=0)\n",
    "\n",
    "        output_neg = self.text_encoder(text_ids_all,\n",
    "                                       attention_mask = text_atts_all,\n",
    "                                       encoder_hidden_states = image_embeds_all,\n",
    "                                       encoder_attention_mask = image_atts_all,\n",
    "                                       return_dict = True,\n",
    "                                      )\n",
    "\n",
    "        vl_embeddings = torch.cat([output_pos.last_hidden_state[:,0,:], output_neg.last_hidden_state[:,0,:]],dim=0)\n",
    "        vl_output = self.itm_head(vl_embeddings)\n",
    "\n",
    "        itm_labels = torch.cat([torch.ones(bs,dtype=torch.long),torch.zeros(2*bs,dtype=torch.long)],\n",
    "                               dim=0).to(image.device)\n",
    "        loss_irm = F.cross_entropy(vl_output, itm_labels)\n",
    "        \n",
    "        ##================= LM ========================##\n",
    "        text.input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "\n",
    "        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)\n",
    "\n",
    "        decoder_targets[:,:self.prompt_length] = -100\n",
    "\n",
    "        decoder_output = self.text_decoder(text.input_ids,\n",
    "                                           attention_mask = text.attention_mask,\n",
    "                                           encoder_hidden_states = image_embeds,\n",
    "                                           encoder_attention_mask = image_atts,\n",
    "                                        #    labels = decoder_targets,\n",
    "                                           return_dict = True,\n",
    "                                          )\n",
    "        # TODO: check real loss\n",
    "        # loss_lm = decoder_output.loss\n",
    "        loss_lm = 0\n",
    "        return loss_irc, loss_irm, loss_lm\n",
    "        \n",
    "    def generate(self, image, knowledge_skg, sample=False, num_beams=3, max_length=90, min_length=10, top_p=0.9, repetition_penalty=1.0):\n",
    "        if self.args.dataset_name == 'iu_xray':\n",
    "            image_embeds0 = self.visual_encoder(image[:, 0])\n",
    "            image_embeds1 = self.visual_encoder(image[:, 1])\n",
    "            image_embeds = torch.cat((image_embeds0, image_embeds1), dim=2)\n",
    "            image_embeds = self.iu_proj(image_embeds)\n",
    "        else:\n",
    "            image_embeds = self.visual_encoder(image)\n",
    "\n",
    "        image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        _, knowledge_words = self.create_knowledge.get_image_knowledge(image.device, image_feat, image_embeds, k=3)\n",
    "\n",
    "        #get each knowledge\n",
    "        bs = image_feat.shape[0]\n",
    "        knowledge_tc_re = []\n",
    "        for i in range(bs):\n",
    "            knowledge_all = ''\n",
    "            for j in range(3):\n",
    "                knowledge_item = knowledge_words[i+bs*j]\n",
    "                knowledge_item = knowledge_item.split('-')\n",
    "                knowledge_item.pop()\n",
    "                knowledge_item = '-'.join(knowledge_item)\n",
    "                if j == 0:\n",
    "                    knowledge_all += knowledge_item\n",
    "                else:\n",
    "                    knowledge_all = knowledge_all + '-' + knowledge_item\n",
    "                # knowledge_all += knowledge_item.replace(knowledge_item.split('-')[-1], ' ')\n",
    "            knowledge_list = knowledge_all.split('-')\n",
    "            # knowledge_list.pop()\n",
    "            knowledge_tc_re.append(knowledge_list)\n",
    "\n",
    "        knowledge_skg['node_inds'] = [each.tolist() for each in knowledge_skg['node_inds']]\n",
    "        knowledge_skg['node_labels'] = [each.tolist() for each in knowledge_skg['node_labels']]\n",
    "        knowledge_skg['node_relations'] = [each.tolist() for each in knowledge_skg['node_relations']]\n",
    "        for i in range(58):\n",
    "            knowledge_skg['node_inds'].append(torch.zeros(bs).tolist())\n",
    "            knowledge_skg['node_labels'].append(torch.zeros(bs).tolist())\n",
    "            knowledge_skg['node_relations'].append(torch.zeros(bs).tolist())\n",
    "        knowledge_skg['node_inds'] = torch.tensor(knowledge_skg['node_inds'])\n",
    "        knowledge_skg['node_labels'] = torch.tensor(knowledge_skg['node_labels'])\n",
    "        knowledge_skg['node_relations'] = torch.tensor(knowledge_skg['node_relations'])\n",
    "\n",
    "        for idx in range(len(knowledge_tc_re)):\n",
    "\n",
    "            knowledge = knowledge_tc_re[idx]\n",
    "            for i in range(len(knowledge)):\n",
    "                knowledge[i] = knowledge[i].strip().replace('located _ at', 'located_at').replace('suggestive _ of', 'suggestive_of')\n",
    "                triplet_item = knowledge[i].split(' ')\n",
    "                if len(triplet_item) != 3:\n",
    "                    triplet = None\n",
    "                    knowledge_skg = update_skg(knowledge_skg, triplet, idx, i)\n",
    "                else:\n",
    "                    triplet = [triplet_item[0], triplet_item[1], triplet_item[2]]\n",
    "                    knowledge_skg = update_skg(knowledge_skg, triplet, idx, i)\n",
    "\n",
    "        for i in range(bs):\n",
    "            knowledge_skg['nodes'][i] = knowledge_skg['nodes'][i].replace('-', ' ')\n",
    "\n",
    "        tag_output = self.tag_encoder(knowledge_skg, image.device)\n",
    "\n",
    "        image_embeds, vis_attn2 = self.cross_attn(image_embeds, tag_output)\n",
    "\n",
    "        image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # bs x 768\n",
    "        \n",
    "        if not sample:\n",
    "            image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)\n",
    "        \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\n",
    "\n",
    "        prompt = [self.prompt] * image.size(0)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device)\n",
    "        input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "        input_ids = input_ids[:, :-1]\n",
    "        if sample:\n",
    "            #nucleus sampling\n",
    "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
    "                                                  max_length=max_length,\n",
    "                                                  min_length=min_length,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_p=top_p,\n",
    "                                                  num_return_sequences=1,\n",
    "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
    "                                                  pad_token_id=self.tokenizer.pad_token_id,\n",
    "                                                  repetition_penalty=1.1,\n",
    "                                                  **model_kwargs)\n",
    "        else:\n",
    "            #beam search\n",
    "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
    "                                                  max_length=max_length,\n",
    "                                                  min_length=min_length,\n",
    "                                                  num_beams=num_beams,\n",
    "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
    "                                                  pad_token_id=self.tokenizer.pad_token_id,\n",
    "                                                  repetition_penalty=repetition_penalty,\n",
    "                                                  **model_kwargs)\n",
    "\n",
    "        captions = []\n",
    "        for output in outputs:\n",
    "            caption = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            captions.append(caption[len(self.prompt):])\n",
    "\n",
    "        return captions, knowledge_words\n",
    "    \n",
    "    \n",
    "    \n",
    "    @torch.no_grad()    \n",
    "    def copy_params(self):\n",
    "        for model_pair in self.model_pairs:           \n",
    "            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n",
    "                param_m.data.copy_(param.data)  # initialize\n",
    "                param_m.requires_grad = False  # not update by gradient    \n",
    "\n",
    "            \n",
    "    @torch.no_grad()        \n",
    "    def _momentum_update(self):\n",
    "        for model_pair in self.model_pairs:           \n",
    "            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n",
    "                param_m.data = param_m.data * self.momentum + param.data * (1. - self.momentum)\n",
    "\n",
    "                        \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, image_feat, text_feat):\n",
    "        # gather keys before updating queue\n",
    "        # image_feats = concat_all_gather(image_feat)\n",
    "        # text_feats = concat_all_gather(text_feat)\n",
    "\n",
    "        batch_size = image_feat.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.queue_size % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.image_queue[:, ptr:ptr + batch_size] = image_feat.T\n",
    "        self.text_queue[:, ptr:ptr + batch_size] = text_feat.T\n",
    "\n",
    "        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr \n",
    "    \n",
    "\n",
    "def blip_decoder(pretrained='',**kwargs):\n",
    "    model = BLIP_Decoder(**kwargs)\n",
    "    if pretrained:\n",
    "        model,msg = load_checkpoint(model,pretrained)\n",
    "    return model    \n",
    "    \n",
    "def init_tokenizer(args):\n",
    "    if args.bert == 'base':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif args.bert == 'sci':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    elif args.bert == 'cli':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})       \n",
    "    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]  \n",
    "    return tokenizer\n",
    "\n",
    "def create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n",
    "    # create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
    "    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n",
    "    if vit=='base':\n",
    "        vision_width = 768\n",
    "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12,\n",
    "                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
    "                                           drop_path_rate=0 or drop_path_rate\n",
    "                                          )\n",
    "        # visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12,\n",
    "        #                                    num_heads=12, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "    elif vit=='large':\n",
    "        vision_width = 1024\n",
    "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n",
    "                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
    "                                           drop_path_rate=0.1 or drop_path_rate\n",
    "                                          )   \n",
    "    return visual_encoder, vision_width\n",
    "\n",
    "def is_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "def load_checkpoint(model,url_or_filename):\n",
    "    if is_url(url_or_filename):\n",
    "        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n",
    "        checkpoint = torch.load(cached_file, map_location='cpu') \n",
    "    elif os.path.isfile(url_or_filename):        \n",
    "        checkpoint = torch.load(url_or_filename, map_location='cpu') \n",
    "    else:\n",
    "        raise RuntimeError('checkpoint url or path is invalid')\n",
    "        \n",
    "    state_dict = checkpoint['model']\n",
    "    \n",
    "    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n",
    "    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n",
    "        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n",
    "                                                                         model.visual_encoder_m)    \n",
    "    for key in model.state_dict().keys():\n",
    "        if key in state_dict.keys():\n",
    "            if state_dict[key].shape!=model.state_dict()[key].shape:\n",
    "                # print(state_dict[key])\n",
    "                print(state_dict[key].shape)\n",
    "                print(model.state_dict()[key].shape)\n",
    "                del state_dict[key]\n",
    "    \n",
    "    msg = model.load_state_dict(state_dict,strict=False)\n",
    "    print('load checkpoint from %s'%url_or_filename)  \n",
    "    return model,msg\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def concat_all_gather(tensor):\n",
    "    \"\"\"\n",
    "    Performs all_gather operation on the provided tensors.\n",
    "    *** Warning ***: torch.distributed.all_gather has no gradient.\n",
    "    \"\"\"\n",
    "    tensors_gather = [torch.ones_like(tensor)\n",
    "        for _ in range(torch.distributed.get_world_size())]\n",
    "    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n",
    "\n",
    "    output = torch.cat(tensors_gather, dim=0)\n",
    "    return output     \n",
    "\n",
    "\n",
    "from typing import List\n",
    "def tie_encoder_decoder_weights(encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, skip_key:str):\n",
    "    uninitialized_encoder_weights: List[str] = []\n",
    "    if decoder.__class__ != encoder.__class__:\n",
    "        logger.info(\n",
    "            f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder weights are correctly initialized.\"\n",
    "        )\n",
    "\n",
    "    def tie_encoder_to_decoder_recursively(\n",
    "        decoder_pointer: nn.Module,\n",
    "        encoder_pointer: nn.Module,\n",
    "        module_name: str,\n",
    "        uninitialized_encoder_weights: List[str],\n",
    "        skip_key: str,\n",
    "        depth=0,\n",
    "    ):\n",
    "        assert isinstance(decoder_pointer, nn.Module) and isinstance(\n",
    "            encoder_pointer, nn.Module\n",
    "        ), f\"{decoder_pointer} and {encoder_pointer} have to be of type torch.nn.Module\"\n",
    "        if hasattr(decoder_pointer, \"weight\") and skip_key not in module_name:\n",
    "            assert hasattr(encoder_pointer, \"weight\")\n",
    "            encoder_pointer.weight = decoder_pointer.weight\n",
    "            if hasattr(decoder_pointer, \"bias\"):\n",
    "                assert hasattr(encoder_pointer, \"bias\")\n",
    "                encoder_pointer.bias = decoder_pointer.bias                \n",
    "            print(module_name+' is tied')    \n",
    "            return\n",
    "\n",
    "        encoder_modules = encoder_pointer._modules\n",
    "        decoder_modules = decoder_pointer._modules\n",
    "        if len(decoder_modules) > 0:\n",
    "            assert (\n",
    "                len(encoder_modules) > 0\n",
    "            ), f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n",
    "\n",
    "            all_encoder_weights = set([module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()])\n",
    "            encoder_layer_pos = 0\n",
    "            for name, module in decoder_modules.items():\n",
    "                if name.isdigit():\n",
    "                    encoder_name = str(int(name) + encoder_layer_pos)\n",
    "                    decoder_name = name\n",
    "                    if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n",
    "                        encoder_modules\n",
    "                    ) != len(decoder_modules):\n",
    "                        # this can happen if the name corresponds to the position in a list module list of layers\n",
    "                        # in this case the decoder has added a cross-attention that the encoder does not have\n",
    "                        # thus skip this step and subtract one layer pos from encoder\n",
    "                        encoder_layer_pos -= 1\n",
    "                        continue\n",
    "                elif name not in encoder_modules:\n",
    "                    continue\n",
    "                elif depth > 500:\n",
    "                    raise ValueError(\n",
    "                        \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is a circular dependency between two or more `nn.Modules` of your model.\"\n",
    "                    )\n",
    "                else:\n",
    "                    decoder_name = encoder_name = name\n",
    "                tie_encoder_to_decoder_recursively(\n",
    "                    decoder_modules[decoder_name],\n",
    "                    encoder_modules[encoder_name],\n",
    "                    module_name + \"/\" + name,\n",
    "                    uninitialized_encoder_weights,\n",
    "                    skip_key,\n",
    "                    depth=depth + 1,\n",
    "                )\n",
    "                all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n",
    "\n",
    "            uninitialized_encoder_weights += list(all_encoder_weights)\n",
    "\n",
    "    # tie weights recursively\n",
    "    tie_encoder_to_decoder_recursively(decoder, encoder, base_model_prefix, uninitialized_encoder_weights, skip_key)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a87e98-2820-4e9d-b8b7-205db1974f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4141080/1760840430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLIP_Decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4141080/3598212243.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, med_config, image_size, vit, vit_grad_ckpt, vit_ckpt_layer, prompt, tokenizer, embed_dim, queue_size, momentum, args)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'base'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mmed_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'configs/med_config_blip.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sci'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "model = BLIP_Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d798812-f23b-4d66-ba38-d005dfc9faab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCL",
   "language": "python",
   "name": "dcl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
